V dynamic learning rate 
	start with 0.01 if acc doesn't improve 5 epochs learn/=2; if learn < (bound = 0.0001): learn = bound
V different opt
	adam
	rmsprop
	adagrad

V hidden units 
	try 300 like static rnn project
V bach size
	tried 100, 200, 300
V change emb file
V multi cell
V enlarge data file
V initializer=tf.contrib.layers.xavier_initializer()

V get different loss funcs
try lr again
learning_rate = tf.train.exponential_decay(
        1e-3,                      # Base learning rate.
        global_step * BATCH_SIZE,  # Current index into the dataset.
        train_size,                # Decay step.
        0.95,                      # Decay rate.
        staircase=True)

attention https://programtalk.com/python-examples/tensorflow.contrib.rnn.AttentionCellWrapper/
bn
drop out layer to each cell
graph experiments

bn outputs_as_value
truncated back propofation through time https://www.tensorflow.org/tutorials/sequences/recurrent#truncated_backpropagation
